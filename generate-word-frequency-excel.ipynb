{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7eccc09-c396-4ff5-8c41-0ef80f3038e9",
   "metadata": {},
   "source": [
    "# Generate Word Frequency in Audit Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa795e-4275-48be-9874-5bbb1dd9ceed",
   "metadata": {},
   "source": [
    "## Steps Taken\n",
    "\n",
    "1. Download the folder with audit reports from OneDrive\n",
    "2. Iterate through the folder and get the folders with each country's reports\n",
    "3. For each country's folder, get the country's reports\n",
    "4. Lemmatize the words in each report\n",
    "5. Get the frequency distribution of each text\n",
    "6. Filter the words to remove noise\n",
    "7. Enter the frequency distribution into a pandas dataframe\n",
    "8. Merge each country's frequency distributions into one dataframe with the words as rows and report name as column\n",
    "9. Enter each country as a spreadsheet in an excel workbook\n",
    "10. Delete the downloaded documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33207fd-7635-44fa-a906-00a0d7e8c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link to audit_reports; publicly accessible\n",
    "audit_reports_link = \"https://stir-my.sharepoint.com/:f:/g/personal/fkc3_stir_ac_uk/Esgp-VMQyzBClY5vpTP9TsYBTCb16iA3NvelLEJM53VEgQ?e=7ejaR3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26882d5-fae7-40c6-8a17-80640e94c52a",
   "metadata": {},
   "source": [
    "### 2. Iterate through the folder and create a dictionary to match each country to files within it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62caa145-bfa8-46fc-902a-525d575d274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdfminer.high_level import extract_text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import functools\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017b0e49-73e6-433a-a0c2-5e3fb308a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_reports_file_path = \"./audit_reports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa87889f-3be2-409f-b301-1bc05dc867f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_audit_report_dict = {}\n",
    "\n",
    "for root, dirs, files in os.walk(audit_reports_file_path):\n",
    "    if (dirs): # ignore the country folders as children\n",
    "        continue\n",
    "    country_audit_report_dict[root] = sorted(files);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caac5c2b-10ed-4246-93a2-ee33e28bf076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reports_as_text():\n",
    "    for country_folder_path, filenames in country_audit_report_dict.items():\n",
    "        for filename in filenames:\n",
    "            file_path = f'{country_folder_path}/{filename}'\n",
    "            text_file_path = file_path.replace('.pdf', '.txt') # text path of pdf\n",
    "            try:\n",
    "                if file_path.endswith('.txt') or os.path.isfile(text_file_path): continue\n",
    "\n",
    "                print(f'Doing {file_path}')               \n",
    "                text = extract_text(f'{file_path}').lower()\n",
    "\n",
    "                with open(text_file_path, 'w') as text_file:\n",
    "                    text_file.write(text)\n",
    "            \n",
    "            except:\n",
    "                print(f'{filename} has a problem')\n",
    "\n",
    "#save_reports_as_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1245c2-6121-45ed-b9d2-87bd7c6c354a",
   "metadata": {},
   "source": [
    "### 3. For each country's folder, get the country's reports as text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d5b15d-2999-4758-a5fb-39e20d0a9567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "country_texts_dict = {}\n",
    "\n",
    "for country_folder_path, filenames in country_audit_report_dict.items():\n",
    "    for filename in filenames:\n",
    "        # print(f'Doing {country_folder_path}/{filename}')\n",
    "        try:\n",
    "            if not filename.endswith('.txt'): continue\n",
    "\n",
    "            with open(f'{country_folder_path}/{filename}') as text:\n",
    "                # print(filename, text.readline())\n",
    "                if country_folder_path in country_texts_dict:\n",
    "                    country_texts_dict[country_folder_path][filename] = text.read();\n",
    "                    \n",
    "                elif country_folder_path not in country_texts_dict:\n",
    "                    country_texts_dict[country_folder_path] = {filename: text.read()}\n",
    "                    \n",
    "        except:\n",
    "            print(f'{filename} has a problem')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464dee1-2a1f-4fcc-b4aa-f493a50744cf",
   "metadata": {},
   "source": [
    "### 4. Lemmatize the words in each report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dcd2fd3-aa14-47aa-9e7e-2acfba5e1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "for country_path in country_texts_dict.keys():\n",
    "    for report_name, report_text in country_texts_dict[country_path].items():\n",
    "        if report_text:\n",
    "            words_in_report = nltk.word_tokenize(report_text.lower()) \n",
    "\n",
    "            words_in_report = [lemmatizer.lemmatize(word) for word in words_in_report]\n",
    "\n",
    "            country_texts_dict[country_path][report_name] = words_in_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d41e0e6-6b45-49e0-9dd9-ae4a6d0156b6",
   "metadata": {},
   "source": [
    "### 5. Get the frequency distribution of each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bed7ed5-af9d-46bc-aa5e-d23105eabeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_report_freq_dist_dict = {}\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# remove words with numbers and those that are stopwords\n",
    "def clean_fdist(fdist):\n",
    "    return {word: freq for word, freq in fdist.items() if word.isalpha() and word not in stopwords_set}\n",
    "\n",
    "for country_path in country_texts_dict.keys():\n",
    "    for report_name, report_words in country_texts_dict[country_path].items():\n",
    "        fdist = nltk.FreqDist(report_words)\n",
    "        fdist = clean_fdist(fdist)\n",
    "\n",
    "        bigram_fdist = nltk.FreqDist(nltk.bigrams(report_words))\n",
    "        # join the words if the words are all alphabets\n",
    "        # and none of the words is a stopword\n",
    "        bigram_fdist = {' '.join(key): value for key, value in bigram_fdist.items() if ''.join(key).isalpha() and set(key).isdisjoint(stopwords_set)}\n",
    "\n",
    "        trigram_fdist = nltk.FreqDist(nltk.trigrams(report_words))\n",
    "        trigram_fdist = {' '.join(key): value for key, value in trigram_fdist.items() if ''.join(key).isalpha() and set(key).isdisjoint(stopwords_set)}\n",
    "        \n",
    "        fdist.update(bigram_fdist)\n",
    "        fdist.update(trigram_fdist)\n",
    "        \n",
    "\n",
    "        if country_path in country_report_freq_dist_dict:\n",
    "            country_report_freq_dist_dict[country_path][report_name] = fdist\n",
    "        else:\n",
    "            country_report_freq_dist_dict[country_path] = {report_name: fdist}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70d1dd-247c-4975-9177-8781f7cef986",
   "metadata": {},
   "source": [
    "### 7. Enter the frequency distributions into one dataframe per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b032ca-7ac2-4ac2-b3d1-103db52891f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./audit_reports/Gambia to file\n",
      "Completed writing to ./audit_reports/Gambia\n",
      "Writing ./audit_reports/Zambia to file\n",
      "Completed writing to ./audit_reports/Zambia\n",
      "Writing ./audit_reports/South-Africa to file\n",
      "Completed writing to ./audit_reports/South-Africa\n",
      "Writing ./audit_reports/Nigeria to file\n",
      "Completed writing to ./audit_reports/Nigeria\n",
      "Writing ./audit_reports/Esthwani to file\n",
      "Completed writing to ./audit_reports/Esthwani\n",
      "Writing ./audit_reports/Kenya to file\n",
      "Completed writing to ./audit_reports/Kenya\n",
      "Writing ./audit_reports/Ghana to file\n",
      "Completed writing to ./audit_reports/Ghana\n",
      "Writing ./audit_reports/Malawi to file\n",
      "Completed writing to ./audit_reports/Malawi\n",
      "Writing ./audit_reports/Tanzania to file\n",
      "Completed writing to ./audit_reports/Tanzania\n",
      "Writing ./audit_reports/Uganda to file\n",
      "Completed writing to ./audit_reports/Uganda\n"
     ]
    }
   ],
   "source": [
    "with pd.ExcelWriter('word-frequency.xlsx', engine='xlsxwriter') as writer:\n",
    "    for country_path in country_report_freq_dist_dict.keys():\n",
    "        df = pd.DataFrame()\n",
    "        print(f'Writing {country_path} to file')\n",
    "    \n",
    "        for report_name, freq_dist in country_report_freq_dist_dict[country_path].items():\n",
    "            if df.empty:\n",
    "                df = pd.DataFrame.from_dict(dict(freq_dist), orient='index', columns=[report_name])\n",
    "            else:\n",
    "                other_df = pd.DataFrame.from_dict(dict(freq_dist), orient='index', columns=[report_name])\n",
    "                df = pd.merge(df, other_df, 'outer', left_index=True, right_index=True)\n",
    "\n",
    "        df.to_excel(writer, sheet_name=f'{country_path.removeprefix(\"./audit_reports/\")}')\n",
    "        print(f'Completed writing to {country_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48dd06a-2f05-45b6-8de7-87f049b4003a",
   "metadata": {},
   "source": [
    "### 8. Filter financial words using AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3e6e45-8bb7-4f06-a549-7907db7a8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_key(secrets_file, key_name):\n",
    "    # retrieves key from secrets_file if saved in the format\n",
    "    #  key_name='example_api_key'\n",
    "    with open(secrets_file) as env_file:\n",
    "        for line in env_file.readlines():\n",
    "            if line.startswith(key_name):\n",
    "                return line.strip().removeprefix(f'{key_name}=')\n",
    "\n",
    "openai_api_key = get_api_key('.env', 'OPENAI_DEV_KEY')\n",
    "\n",
    "def send_prompt_to_GPT(instruction, background_context=None):\n",
    "    \n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": background_context},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "      ]\n",
    "    )\n",
    "    #print(completion.choices[0].message.content)\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb76750-044e-4a97-b947-1b89537bf722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3c11d7a-50dc-4a1f-8c19-41e044fd92c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['', 'and council matter', 'public account', 'hospital general',\\n       'revealed poor record', 'failure inscribe government', 'macroeconomics',\\n       'inspection section', 'billed', 'justice wa',\\n       ...\\n       'grz sponsored student', 'revenue undercharging', 'requisition',\\n       'comprehensive maintenance', 'bursary', 'factory', 'hospital mental',\\n       'amount capitalization', 'failure follow', 'appendix iii summary'],\\n      dtype='object', length=5753)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m             dataframe\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39msheet_name)\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone writing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msheet_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m spreadsheet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m gambia_words \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_non_finance_words\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword-frequency.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(gambia_words))\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# [print(word) for word in gambia_words]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m, in \u001b[0;36mfilter_non_finance_words\u001b[0;34m(excel_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m country_finance_words_generator \u001b[38;5;241m=\u001b[39m send_each_countrys_words_to_GPT(country_words_generator)\n\u001b[1;32m      8\u001b[0m filtered_dataframe_generator \u001b[38;5;241m=\u001b[39m filter_finance_words_in_dataframe(country_freq_dists_generator, country_finance_words_generator)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mwrite_dataframes_to_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfiltered-word-frequency.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered_dataframe_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcel_sheetnames\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 65\u001b[0m, in \u001b[0;36mwrite_dataframes_to_excel\u001b[0;34m(filename, dataframe_generator, sheet_names)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_dataframes_to_excel\u001b[39m(filename, dataframe_generator, sheet_names):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mExcelWriter(filename, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlsxwriter\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m---> 65\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m dataframe, sheet_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dataframe_generator, sheet_names):\n\u001b[1;32m     66\u001b[0m             dataframe\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39msheet_name)\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone writing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msheet_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m spreadsheet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 61\u001b[0m, in \u001b[0;36mfilter_finance_words_in_dataframe\u001b[0;34m(dataframe_generator, finance_words_generator)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_finance_words_in_dataframe\u001b[39m(dataframe_generator, finance_words_generator):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataframe, finance_words \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dataframe_generator, finance_words_generator):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# for dataframe in dataframe_generator:\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinance_words\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/auditor-general-reports-analyses/env/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/projects/auditor-general-reports-analyses/env/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/projects/auditor-general-reports-analyses/env/lib/python3.10/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['', 'and council matter', 'public account', 'hospital general',\\n       'revealed poor record', 'failure inscribe government', 'macroeconomics',\\n       'inspection section', 'billed', 'justice wa',\\n       ...\\n       'grz sponsored student', 'revenue undercharging', 'requisition',\\n       'comprehensive maintenance', 'bursary', 'factory', 'hospital mental',\\n       'amount capitalization', 'failure follow', 'appendix iii summary'],\\n      dtype='object', length=5753)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# handle the process of filtering the words\n",
    "def filter_non_finance_words(excel_name):\n",
    "    with pd.ExcelFile(excel_name) as excel_file:\n",
    "        excel_sheetnames = get_sheet_names(excel_file)\n",
    "        country_freq_dists_generator = gen_freq_dists(excel_sheetnames, excel_file)\n",
    "        country_words_generator = gen_each_countrys_words(country_freq_dists_generator)\n",
    "        country_finance_words_generator = send_each_countrys_words_to_GPT(country_words_generator)\n",
    "        filtered_dataframe_generator = filter_finance_words_in_dataframe(country_freq_dists_generator, country_finance_words_generator)\n",
    "        write_dataframes_to_excel('filtered-word-frequency.xlsx', filtered_dataframe_generator, excel_sheetnames)\n",
    "            \n",
    "        # return next(country_finance_words_generator)\n",
    "\n",
    "def get_sheet_names(excel_file):\n",
    "    return excel_file.sheet_names\n",
    "\n",
    "def gen_freq_dists(excel_sheetnames, excel_file):\n",
    "    for name in excel_sheetnames:\n",
    "        # yield the dataframe so they are not all saved to memory\n",
    "        yield pd.read_excel(excel_file, name, index_col=0)\n",
    "\n",
    "def gen_each_countrys_words(freq_dist_generator):\n",
    "    for dataframe in freq_dist_generator:\n",
    "        yield list(dataframe.index)\n",
    "\n",
    "def send_each_countrys_words_to_GPT(country_words_generator):\n",
    "    array_of_batch = None\n",
    "    system_instruction = create_system_instruction()\n",
    "    for country_words in country_words_generator:\n",
    "        array_of_batch = split_words(country_words)\n",
    "\n",
    "        # send each batch of strings to gpt\n",
    "        # convert the result to json\n",
    "        # reduce the arrays into one\n",
    "\n",
    "        finance_array_of_batch = [send_prompt_to_GPT(system_instruction, json.dumps(batch)) for batch in array_of_batch]\n",
    "        finance_strings = functools.reduce(lambda x, y: x + y, finance_array_of_batch, '')\n",
    "        yield set(re.findall(r\"'(.*?)'\", finance_strings))\n",
    "\n",
    "def split_words(country_words):\n",
    "    max_limit = 500;\n",
    "    if len(country_words) > max_limit:\n",
    "        return [country_words[i:i + max_limit] for i in range(0, len(country_words), max_limit)]\n",
    "    else:\n",
    "        return [country_words]\n",
    "\n",
    "def create_system_instruction():\n",
    "    return (\"You'll receive a list containing strings. \"\n",
    "            \"Check each string for words related to the industry of finance, accounting or public administration. \"\n",
    "            \"Some words are lower-case acronyms related to the topics previously mentioned \"\n",
    "            \"such as ifmis, gifmis, ipsas, ifrs etc. \"\n",
    "            \"If a string has at least one word matching the criteria, return it.\"\n",
    "            \"For example, return a string of the format \\\"'ifrs' 'financial statement' 'ifrs financial statement'\\\" and so forth\"\n",
    "            \"Basically, you are filtering out words that don't fit the criteria I've given you\"\n",
    "            \"I expect the strings in single quotation marks separated by spaces. Don't add markup or comments.\")\n",
    "\n",
    "def filter_finance_words_in_dataframe(dataframe_generator, finance_words_generator):\n",
    "    for dataframe, finance_words in zip(dataframe_generator, finance_words_generator):\n",
    "    # for dataframe in dataframe_generator:\n",
    "        yield dataframe[list(finance_words)]\n",
    "\n",
    "def write_dataframes_to_excel(filename, dataframe_generator, sheet_names):\n",
    "    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:\n",
    "        for dataframe, sheet_name in zip(dataframe_generator, sheet_names):\n",
    "            dataframe.to_excel(writer, sheet_name=sheet_name)\n",
    "            print(f'Done writing {sheet_name} spreadsheet')\n",
    "\n",
    "filter_non_finance_words('word-frequency.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd701f49-9c3b-4fa8-854a-e4ac7fe1b9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accounting a\n",
      "financial statement b\n"
     ]
    }
   ],
   "source": [
    "ti = '[\"accounting\", \"financial statement\"]'\n",
    "t = json.loads(ti)\n",
    "bi = ['a', 'b']\n",
    "\n",
    "for a, b in zip(t, bi):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3b66ce6-4df1-43a3-b640-513b7ff07cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (\"You'll receive an list containing strings. \"\n",
    "            \"Check each string for words related to finance, accounting or public administration. \"\n",
    "            \"Some words are lower-case acronyms related to the topics previously mentioned \"\n",
    "            \"such as ifmis, gifmis, ipsas, ifrs etc. \"\n",
    "            \"If a string has at least one word matching the criteria, add it into an array.\"\n",
    "            \"For example, if given, ['ifrs', 'financial statement', 'walls painted'], \"\n",
    "            \"return \\\"'ifrs' 'financial statement'\\\"\"\n",
    "            \"Basically, you are filtering out words that don't fit the criteria I've given you\"\n",
    "            \"I expect only a valid JSON list back in the form of a string. Don't add markup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e9ed79e-ca4d-49e1-a3f2-3c056f96bb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ifrs', 'financial statement', 'public administration']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "s = \"'ifrs' 'financial statement' 'public administration'\"\n",
    "result = re.findall(r\"'(.*?)'\", s)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730fc63-ca99-48cf-a6dd-b50a64a6dbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auditor-general-reports-analyses",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
